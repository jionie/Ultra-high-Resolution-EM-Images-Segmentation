{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install -U git+https://github.com/albu/albumentations\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import fbeta_score, f1_score, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time \n",
    "import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import torch.nn.utils.weight_norm as weightNorm\n",
    "import torch.nn.init as init\n",
    "\n",
    "import cv2\n",
    "import json \n",
    "import gc\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations import pytorch as AT\n",
    "\n",
    "import smp_local as smp  \n",
    "\n",
    "import scipy.special\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "sigmoid = lambda x: scipy.special.expit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 300\n",
    "base_dir = '../input/'\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fb63b9c30516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/jionie/my_disk/Kaggle/URES/input/URES/U-RISC OPEN DATA SIMPLE/U-RISC OPEN DATA SIMPLE/train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/media/jionie/my_disk/Kaggle/URES/input/URES/U-RISC OPEN DATA SIMPLE/U-RISC OPEN DATA SIMPLE/labels/train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "data_folder = '../input/train/'\n",
    "label_folder = '../input/labels/train/'\n",
    "data_list = list(map(lambda x: x.replace('.png', ''), os.listdir(data_folder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, val = train_test_split(data_list, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = list(map(lambda x: data_folder+x+'.png', tr))\n",
    "data_tr_gt = list(map(lambda x: label_folder+x+'.tiff', tr))\n",
    "\n",
    "data_val = list(map(lambda x: data_folder+x+'.png', val))\n",
    "data_val_gt = list(map(lambda x: label_folder+x+'.tiff', val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellDataset(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_list, \n",
    "            mask_list, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "\n",
    "        self.images_list = images_list\n",
    "        self.mask_list = mask_list\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_list[i])        \n",
    "        mask = 1.0 - cv2.imread(self.mask_list[i], 0)/255.\n",
    "        mask = mask[:,:,np.newaxis]\n",
    "                \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "    \n",
    "    \n",
    "class CellDatasetInference(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_list, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "\n",
    "        self.images_list = images_list\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_list[i]) \n",
    "\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image)\n",
    "            image= sample['image']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "            \n",
    "        return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show train set\n",
    "\n",
    "dataset = CellDataset(data_tr, data_tr_gt)\n",
    "\n",
    "image, mask = dataset[4] # get some sample\n",
    "visualize(\n",
    "    image=image, \n",
    "    mask=mask.squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        \n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        \n",
    "#         albu.ShiftScaleRotate(scale_limit=0.25, rotate_limit=0, shift_limit=0.25, p=0.5),\n",
    "        \n",
    "#         albu.RandomResizedCrop(1024, 1024, scale=(0.4, 1.0), ratio=(0.75, 1.33),always_apply=True),\n",
    "#         albu.RandomCrop(1024,1024,always_apply=True),\n",
    "        \n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "                albu.JpegCompression(quality_lower=20, quality_upper=90, p=1),\n",
    "                albu.RandomContrast(p=1),\n",
    "                #albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(1024, 1024),\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Visualize resulted augmented images and masks\n",
    "\n",
    "augmented_dataset = CellDataset(data_tr, data_tr_gt, augmentation=get_training_augmentation())\n",
    "\n",
    "# same image with different random transforms\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[25]\n",
    "    visualize(image=image, mask=mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    __name__ = 'FocalLoss'\n",
    "    def __init__(self, alpha=0.25, gamma=2, weight=None, ignore_index=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.bce_fn = nn.BCEWithLogitsLoss(weight=self.weight)\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        if self.ignore_index is not None:\n",
    "            mask = labels != self.ignore_index\n",
    "            labels = labels[mask]\n",
    "            preds = preds[mask]\n",
    "\n",
    "        logpt = -self.bce_fn(preds, labels)\n",
    "        pt = torch.exp(logpt)\n",
    "        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1 - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def hinge(pred, label):\n",
    "    signs = 2 * label - 1\n",
    "    errors = 1 - pred * signs\n",
    "    return errors\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels, ignore_index):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore_index: label to ignore\n",
    "    \"\"\"\n",
    "    logits = logits.contiguous().view(-1)\n",
    "    labels = labels.contiguous().view(-1)\n",
    "    \n",
    "    errors = hinge(logits, labels)\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.elu(errors_sorted) + 1, grad)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class LovaszLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore_index: label to ignore\n",
    "    \"\"\"\n",
    "    __name__ = 'LovaszLoss'\n",
    "    def __init__(self, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return lovasz_hinge_flat(logits, labels, self.ignore_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftDiceLoss_binary(nn.Module):\n",
    "    __name__ = 'SoftDiceLoss'\n",
    "    def __init__(self):\n",
    "        super(SoftDiceLoss_binary, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        smooth = 0.01\n",
    "        batch_size = input.size(0)\n",
    "        input = F.sigmoid(input).view(batch_size, -1)\n",
    "        # print(target.shape)\n",
    "        # print(target.view(-1))\n",
    "        target = target.clone().view(batch_size, -1)\n",
    "\n",
    "        inter = torch.sum(input * target, 1) + smooth\n",
    "        union = torch.sum(input * input, 1) + torch.sum(target * target, 1) + smooth\n",
    "\n",
    "        score = torch.sum(2.0 * inter / union) / float(batch_size)\n",
    "        score = 1.0 - torch.clamp(score, 0.0, 1.0 - 1e-7)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBCELoss(nn.Module):\n",
    "    __name__ = 'WeightedBCELoss'\n",
    "    def __init__(self, weight):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        c = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.weight))\n",
    "        return c(input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER = 'resnext101_32x8d'\n",
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "#ACTIVATION = 'sigmoid'\n",
    "ACTIVATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=1, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "model.cuda()\n",
    "model = nn.DataParallel(model)\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CellDataset(\n",
    "    data_tr, \n",
    "    data_tr_gt, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "valid_dataset = CellDataset(\n",
    "    data_val, \n",
    "    data_val_gt, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=8)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def CosineAnnealingWarmUpRestarts(epoch, T_0, T_warmup=0, gamma=1.,):\n",
    "    stage = epoch//T_0\n",
    "    max_lr = gamma**(stage)\n",
    "    res = epoch  - stage*T_0\n",
    "    if epoch < 0:\n",
    "        return 1\n",
    "    elif res <= T_warmup:\n",
    "        return ((0.95/T_warmup)*res+0.05)*max_lr\n",
    "    else:\n",
    "        return (0.05*max_lr + 0.95*max_lr * (1 + math.cos(math.pi * (res-T_warmup)/(T_0-T_warmup))) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lr = lambda x: CosineAnnealingWarmUpRestarts(x, 100, 15, 0.6)\n",
    "plt.plot(list(range(600)),list(map(lr, range(600))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "#loss = smp.utils.losses.BCEDiceLoss(eps=1.)\n",
    "#loss = SoftDiceLoss_binary()\n",
    "#loss = FocalLoss()\n",
    "loss = WeightedBCELoss(3)\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoUMetric(eps=1.),\n",
    "    smp.utils.metrics.FscoreMetric(eps=1.),\n",
    "    smp.utils.metrics.PrecisionMetric(),\n",
    "]\n",
    "\n",
    "lr = 6e-3\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.module.decoder.parameters(), 'lr': lr, 'weight_decay': 0.01}, \n",
    "    {'params': model.module.encoder.parameters(), 'lr': lr*0.01}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp.utils.metrics.PrecisionMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    "    accumulation_steps = 6,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model \n",
    "\n",
    "max_score = 0\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    \n",
    "    lr_rate = CosineAnnealingWarmUpRestarts(i, T_0=100, T_warmup=15, gamma=0.9,)\n",
    "    \n",
    "    optimizer.param_groups[0]['lr'] = lr_rate * lr\n",
    "    optimizer.param_groups[1]['lr'] = lr_rate * lr * 0.01\n",
    "        \n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    seed_everything(seed=SEED+i)\n",
    "    \n",
    "    if i%5 == 0:\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "        # do something (save model, change lr, etc.)\n",
    "        if max_score < valid_logs['precision']:\n",
    "            max_score = valid_logs['precision']\n",
    "            torch.save(model.module, './best_model.pth')\n",
    "            print('Model saved!')\n",
    "            \n",
    "torch.save(model.module, './end_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "best_model = torch.load('./best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_dataset = CellDataset(\n",
    "    data_val, \n",
    "    data_val_gt, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)\n",
    "\n",
    "# best :BCEDice Loss 0.535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset without transformations for image visualization\n",
    "test_dataset_vis = CellDataset(\n",
    "    data_val, data_val_gt, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = pr_mask.squeeze().cpu().numpy()\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=sigmoid(pr_mask)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(mask = sigmoid(pr_mask)>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历阈值求分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = gt_mask.reshape(1024*1024)\n",
    "b = (sigmoid(pr_mask)>0.5).reshape(1024*1024)\n",
    "accuracy_score(np.array(a,dtype=int), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference dataset\n",
    "\n",
    "data_inference_dir = '../input/val/'\n",
    "data_inference_names = os.listdir(data_inference_dir)\n",
    "\n",
    "data_inference = list(map(lambda x: data_inference_dir+x, data_inference_names))\n",
    "\n",
    "inference_dataset = CellDatasetInference(\n",
    "    data_inference, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_inference_names)):\n",
    "    image = inference_dataset[i]\n",
    "    name = data_inference_names[i]\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = pr_mask.squeeze().cpu().numpy()\n",
    "        \n",
    "    save = sigmoid(pr_mask)>0.5\n",
    "    save = (1 - save)*255\n",
    "    cv2.imwrite('result/'+name.replace('.png','.tiff'), save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
